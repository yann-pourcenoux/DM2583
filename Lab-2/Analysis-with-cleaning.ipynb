{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Sentiment Classification with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re, string\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/lab_train.txt\", engine=\"python\")\n",
    "df_train.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "df_train.head() # 173 positive evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"data/lab_test.txt\", engine=\"python\")\n",
    "df_test.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "df_test.head() # 88 positive evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.read_excel('data/evaluation_dataset.xlsx', header=None, names=['review'])\n",
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews = df_train.review.values\n",
    "test_reviews = df_test.review.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data\n",
    "### Using the Natural Language Processing Toolkit package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "def clean_review(tokens, stop_words = stop_words, numbers=True):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "        \n",
    "    for token, tag in pos_tag(tokens):\n",
    "        # Removing links\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        # Removing @\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "            \n",
    "            # remove tokens containing numbers\n",
    "    if numbers:\n",
    "        numbers = [str(i) for i in range(10)]\n",
    "        kill_list = []\n",
    "        for number in numbers:\n",
    "            kill_list = kill_list + [w for w in cleaned_tokens if number in w]\n",
    "        # removing selected tokens\n",
    "        cleaned_tokens = [w for w in cleaned_tokens if not w in kill_list]\n",
    "        \n",
    "        # merge tokens\n",
    "        merged = ' '\n",
    "        merged = merged.join(cleaned_tokens)\n",
    "        \n",
    "    return merged\n",
    "\n",
    "def clean(array):\n",
    "    for i, phrase in enumerate(array):\n",
    "        array[i] = clean_review(word_tokenize(phrase))\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews = clean(train_reviews)\n",
    "test_reviews = clean(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction \n",
    "### Using the scikit-learn librairy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=3,\n",
    "                             stop_words=stopwords.words('english'),\n",
    "                             strip_accents='ascii')\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_reviews).toarray()\n",
    "X_test = vectorizer.transform(test_reviews).toarray()\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def targets(score, thresh=2.5):\n",
    "    targets = np.ones(score.shape, dtype=np.int)\n",
    "    targets = targets - 2*((score<thresh).astype(dtype=np.int))\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = targets(df_train.score.values)\n",
    "Y_test = targets(df_test.score.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "### Since we faced the problem of having only positive predictions when we were using all the data, we chose to limit the influence of the positively labeled data by training the model on a subset of data containing as much positively than negatively labeled data.\n",
    "### This is why there is a for loop, we are trying different subset to train models and keeping only the best model ie the one that yielded the highest f1-score. This scores takes into account the good classification of both 'negative' and 'positive' classes. Not only the accuracy. This is why we get at best 84% accuracy (which is lower than 88% if we would predict only positive values) but this model predicts both good 'negative' and 'positive' labels (see recall score below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = svm.SVC(kernel='rbf')\n",
    "classifier.fit(X_train, Y_train)\n",
    "classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to reduce influence of positive classification\n",
    "neg = Y_train == -1\n",
    "pos = Y_train == 1\n",
    "\n",
    "classifiers=[]\n",
    "scores = []\n",
    "accuracy = []\n",
    "for k in tqdm(range(5000)):\n",
    "    idxs = choice(173, 27, replace=False)\n",
    "    \n",
    "    x_array = [X_train[neg]]+[X_train[idxs]]\n",
    "    y_array = [Y_train[neg]]+[Y_train[idxs]]\n",
    "    \n",
    "    new_x_train = np.concatenate(x_array)\n",
    "    new_y_train = np.concatenate(y_array)\n",
    "    \n",
    "    classifier = svm.SVC(kernel='rbf')\n",
    "    classifier.fit(new_x_train, new_y_train)\n",
    "    Y_pred = classifier.predict(X_test)\n",
    "    \n",
    "#     X_eval = vectorizer.transform(df_eval.review.values)\n",
    "#     score = classifier.predict(X_eval)\n",
    "    \n",
    "    classifiers.append(classifier)\n",
    "    scores.append(f1_score(Y_test,Y_pred))\n",
    "    accuracy.append(np.mean(Y_pred==Y_test))\n",
    "    \n",
    "print('F1-score of the best model', scores[scores.index(max(scores))])\n",
    "print('Accuracy score of the best model', accuracy[scores.index(max(scores))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the best model\n",
    "classifier = classifiers[scores.index(max(scores))]\n",
    "Y_pred = classifier.predict(X_test)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eval = vectorizer.transform(clean(df_eval.review.values)).toarray()\n",
    "pred_eval = classifier.predict(X_eval)\n",
    "print(str(np.sum([pred_eval==1]))+ ' positive ratings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.zeros(pred_eval.shape, dtype='O')\n",
    "predictions[pred_eval==1] = 'positive'\n",
    "predictions[pred_eval==-1] = 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.read_excel('data/evaluation_dataset.xlsx', header=None, names=['review'])\n",
    "df_eval.insert(1, \"prediction\", predictions, True)\n",
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.to_csv('data/evaluation_cleaning.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('data/test.csv', engine='python')\n",
    "X = (df_test.score.values).astype(np.int)[:100]\n",
    "X[X==0]=-1\n",
    "pred = classifier.predict(vectorizer.transform(clean(df_test.review.values)).toarray()[:100])\n",
    "print(classification_report(X, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
