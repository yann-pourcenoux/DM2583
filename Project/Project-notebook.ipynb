{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import gzip\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re, string\n",
    "\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "def clean_single(tweet, stop_words = stopwords.words('english'), numbers=True):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "    # Removing links\n",
    "    tweet = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', tweet)\n",
    "    # Removing @\n",
    "    tweet = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", tweet)\n",
    "        \n",
    "    # Removing currencies\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    \n",
    "    tokens = word_tokenize(tweet)\n",
    "    \n",
    "    for token, tag in pos_tag(tokens):\n",
    "                \n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "            \n",
    "            # remove tokens containing numbers\n",
    "    if numbers:\n",
    "        numbers = [str(i) for i in range(10)]\n",
    "        kill_list = []\n",
    "        for number in numbers:\n",
    "            kill_list = kill_list + [w for w in cleaned_tokens if number in w]\n",
    "        # removing selected tokens\n",
    "        cleaned_tokens = [w for w in cleaned_tokens if not w in kill_list]\n",
    "        \n",
    "        # merge tokens\n",
    "        merged = ' '\n",
    "        merged = merged.join(cleaned_tokens)\n",
    "        \n",
    "    return merged\n",
    "\n",
    "def clean(array):\n",
    "    for i, phrase in enumerate(array):\n",
    "        array[i] = clean_single(phrase)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening dataset files, cleaning and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raw_sentiment = pd.read_csv('data/training.1600000.processed.noemoticon.csv', \n",
    "#                           engine='python', \n",
    "#                           header=None, \n",
    "#                           names=['score', 'tweet_id', 'date', '?', 'account', 'text'])\n",
    "# df_sentiment = df_raw_sentiment[['score', 'text']]\n",
    "\n",
    "\n",
    "# classification = np.ones(len(df_sentiment), dtype=np.int)\n",
    "# classification[df_sentiment['score']==0] = -1\n",
    "\n",
    "# df_sentiment.insert(loc=2, column='classification', value=classification)\n",
    "# df_sentiment = df_sentiment[['classification', 'text']]\n",
    "\n",
    "# sentiment_classification = df_sentiment.classification.values\n",
    "# sentiment_text = clean(df_sentiment.text.values)\n",
    "\n",
    "# print(sentiment_classification.shape, sentiment_text.shape)\n",
    "\n",
    "# temp = pd.DataFrame({'text':sentiment_text,'classification':sentiment_classification})\n",
    "# temp.to_csv('data/sentiment_cleaned.csv', index=False)\n",
    "\n",
    "# df_raw_airlines = pd.read_csv('data/Tweets.csv', engine='python')\n",
    "# df_airlines = df_raw_airlines[['airline_sentiment', 'text']]\n",
    "\n",
    "# classification = np.zeros(len(df_airlines), dtype=np.int)\n",
    "# classification[df_airlines['airline_sentiment']=='negative'] = -1\n",
    "# classification[df_airlines['airline_sentiment']=='neutral'] = 0\n",
    "# classification[df_airlines['airline_sentiment']=='positive'] = 1\n",
    "\n",
    "# df_airlines.insert(loc=2, column='classification', value=classification)\n",
    "# df_airlines = df_airlines[['classification', 'text']]\n",
    "\n",
    "# airlines_classification = df_airlines.classification.values\n",
    "# airlines_text = clean(df_airlines.text.values)\n",
    "\n",
    "# print(airlines_classification.shape, airlines_text.shape)\n",
    "\n",
    "# temp = pd.DataFrame({'text':airlines_text,'classification':airlines_classification})\n",
    "# temp.to_csv('data/airlines_cleaned.csv', index=False)\n",
    "\n",
    "# temp = pd.read_json(r'data/Movies_and_TV_5.json', lines=True)\n",
    "# temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse(path):\n",
    "#     g = gzip.open(path, 'r')\n",
    "#     for l in g:\n",
    "#         yield eval(l)\n",
    "\n",
    "# reviews = []\n",
    "# scores = []\n",
    "# for review in parse('data/reviews_Movies_and_TV_5.json.gz'):\n",
    "#     reviews.append(clean_single(review['reviewText']))\n",
    "#     scores.append(review['overall'])\n",
    "\n",
    "# print(len(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opening cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airlines = pd.read_csv('data/airlines_cleaned.csv').dropna()\n",
    "df_airlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment = pd.read_csv('data/sentiment_cleaned.csv').dropna()\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment repartition over the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1,2,1)\n",
    "labels = ['positive', 'neutral', 'negative']\n",
    "sizes = [np.sum(df_airlines.classification==1), np.sum(df_airlines.classification==0), np.sum(df_airlines.classification==-1)]\n",
    "colors = ['green', 'yellow', 'red']\n",
    "patches, texts = plt.pie(sizes, colors=colors, shadow=True, startangle=140)\n",
    "plt.legend(patches, labels, loc=\"best\")\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.title('Sentiment distribution on airlines dataset')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "labels = ['positive', 'negative']\n",
    "sizes = [np.sum(df_sentiment.classification==1), np.sum(df_sentiment.classification==-1)]\n",
    "colors = ['green', 'red']\n",
    "patches, texts = plt.pie(sizes, colors=colors, shadow=True, startangle=90)\n",
    "plt.legend(patches, labels, loc=\"best\")\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.title('Sentiment distribution on sentiment dataset')\n",
    "plt.savefig('figures/pie_charts_sentiment_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordclouds for the airlines dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "pos_text = \" \".join(df_airlines[df_airlines.classification==1].text.values)\n",
    "neg_text = \" \".join(df_airlines[df_airlines.classification==-1].text.values)\n",
    "neu_text = \" \".join(df_airlines[df_airlines.classification==0].text.values)\n",
    "\n",
    "wordcloud_pos = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(pos_text)\n",
    "wordcloud_neg = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(neg_text)\n",
    "wordcloud_neu = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(neu_text)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(wordcloud_pos, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title('Positive reviews')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(wordcloud_neu, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title('Neutral reviews')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(wordcloud_neg, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title('Negative reviews')\n",
    "plt.savefig('figures/airline_wordclouds.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordclouds for the sentiment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_text = \" \".join(df_sentiment[df_sentiment.classification==1].text.values)\n",
    "neg_text = \" \".join(df_sentiment[df_sentiment.classification==-1].text.values)\n",
    "\n",
    "wordcloud_pos = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(pos_text)\n",
    "wordcloud_neg = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(neg_text)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(wordcloud_pos, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title('Positive reviews')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(wordcloud_neg, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title('Neutral reviews')\n",
    "plt.savefig('figures/sentiment_wordclouds.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TF-IDF Vectorizer without bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=5,\n",
    "                            norm='l2',\n",
    "                            ngram_range=(1,1))\n",
    "\n",
    "x_train = vectorizer.fit_transform(df_airlines.text.values).toarray()\n",
    "y_train = df_airlines.classification.values\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer with bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_bow = TfidfVectorizer(min_df=5,\n",
    "                                norm='l2',\n",
    "                                ngram_range=(1,3))\n",
    "\n",
    "x_train_bow = vectorizer_bow.fit_transform(df_airlines.text.values).toarray()\n",
    "x_train_bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "start = time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train), total=4):\n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(x_train[train_index], y_train[train_index])\n",
    "    scores.append(classifier.score(x_train[test_index], y_train[test_index]))\n",
    "    \n",
    "np.mean(np.asarray(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train_bow), total=4):\n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(x_train_bow[train_index], y_train[train_index])\n",
    "    scores.append(classifier.score(x_train_bow[test_index], y_train[test_index]))\n",
    "    \n",
    "np.mean(np.asarray(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train), total=4):\n",
    "    classifier = SVC()\n",
    "    classifier.fit(x_train[train_index], y_train[train_index])\n",
    "    scores.append(classifier.score(x_train[test_index], y_train[test_index]))\n",
    "    \n",
    "np.mean(np.asarray(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train_bow), total=4):\n",
    "    classifier = SVC()\n",
    "    classifier.fit(x_train_bow[train_index], y_train[train_index])\n",
    "    scores.append(classifier.score(x_train_bow[test_index], y_train[test_index]))\n",
    "    \n",
    "np.mean(np.asarray(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "tmp = time()\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train), total=4):\n",
    "    classifier = RandomForestRegressor(n_estimators=50)\n",
    "    classifier.fit(x_train[train_index], y_train[train_index])\n",
    "    scores.append(classifier.score(x_train[test_index], y_train[test_index]))\n",
    "    \n",
    "np.mean(np.asarray(scores)), time()-tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = time()\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train_bow), total=4):\n",
    "    classifier = RandomForestRegressor(n_estimators=50)\n",
    "    classifier.fit(x_train_bow[train_index], y_train[train_index])\n",
    "    scores.append(classifier.score(x_train_bow[test_index], y_train[test_index]))\n",
    "    \n",
    "np.mean(np.asarray(scores)), time()-tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train), total=4):\n",
    "    classifier = AdaBoostClassifier()\n",
    "    classifier.fit(x_train[train_index], y_train[train_index])\n",
    "    scores.append(classifier.score(x_train[test_index], y_train[test_index]))\n",
    "    \n",
    "np.mean(np.asarray(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train_bow), total=4):\n",
    "    classifier = AdaBoostClassifier()\n",
    "    classifier.fit(x_train_bow[train_index], y_train[train_index])\n",
    "    scores.append(classifier.score(x_train_bow[test_index], y_train[test_index]))\n",
    "    \n",
    "np.mean(np.asarray(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully connected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to one_hot\n",
    "def one_hot(x):\n",
    "    classes = np.unique(x)\n",
    "    array = np.zeros((*x.shape, classes.shape[0]), dtype=np.int)\n",
    "    for i, classe in enumerate(classes):\n",
    "        vector = np.zeros((1,classes.shape[0]), dtype=np.int)\n",
    "        vector[:,i]=1\n",
    "        array[x==classe] = vector\n",
    "    return classes, array\n",
    "\n",
    "classes, y_train = one_hot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, InputLayer, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy, Precision, Recall, AUC\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def FCN(x_train, y_train, x_test, y_test, nodes_1=2000, nodes_2=None):\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(x_train.shape[1])))\n",
    "    model.add(Dense(units=nodes_1,\n",
    "                    activation='elu',\n",
    "                    kernel_initializer='lecun_normal'))\n",
    "    model.add(Dropout(rate=0.4))\n",
    "    \n",
    "    if nodes_2:\n",
    "        model.add(Dense(units=nodes_2,\n",
    "                    activation='elu',\n",
    "                    kernel_initializer='lecun_normal'))\n",
    "        model.add(Dropout(rate=0.4))\n",
    "    \n",
    "    model.add(Dense(units=y_train.shape[1], \n",
    "                    activation='softmax', \n",
    "                    kernel_initializer='lecun_normal'))\n",
    "    \n",
    "    METRICS = [CategoricalAccuracy(name='accuracy'),\n",
    "               Precision(name='precision'),\n",
    "               Recall(name='recall'),\n",
    "               AUC(name='auc')]\n",
    "\n",
    "    optimizer = Adam(lr=1e-5)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=METRICS)\n",
    "    model.summary()\n",
    "    \n",
    "    earlystop = EarlyStopping(monitor='val_loss',\n",
    "                             patience=15,\n",
    "                             restore_best_weights=True)\n",
    "    reduceLR = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                factor=np.sqrt(1e-1),\n",
    "                                verbose=1,\n",
    "                                patience=5)\n",
    "    history = model.fit(x=x_train,\n",
    "                        y=y_train,\n",
    "                        batch_size=32,\n",
    "                        epochs=10000,\n",
    "                        verbose=0,\n",
    "                        callbacks=[earlystop, reduceLR],\n",
    "                        validation_split=0.3,\n",
    "                        shuffle=True,\n",
    "                        workers=2)\n",
    "    \n",
    "    return model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train_bow), total=4):\n",
    "    scores.append(FCN(x_train[train_index], \n",
    "                        y_train[train_index],\n",
    "                        x_train[test_index],\n",
    "                        y_train[test_index],\n",
    "                        nodes_1=50,\n",
    "                        nodes_2=50))\n",
    "\n",
    "print(np.mean(np.asarray(scores), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train_bow), total=4):\n",
    "    scores.append(FCN(x_train_bow[train_index], \n",
    "                      y_train[train_index],\n",
    "                      x_train_bow[test_index],\n",
    "                      y_train[test_index],\n",
    "                      nodes_1=50,\n",
    "                      nodes_2=None))\n",
    "    \n",
    "print(np.mean(np.asarray(scores), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of comparison of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time spent on model tuning :', time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment140 dataset (tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon book reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "107px",
    "width": "241px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
