{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import gzip\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re, string\n",
    "\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "def clean_single(tweet, stop_words = stopwords.words('english'), numbers=True):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "    # Removing links\n",
    "    tweet = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', tweet)\n",
    "    # Removing @\n",
    "    tweet = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", tweet)\n",
    "        \n",
    "    # Removing currencies\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    \n",
    "    tokens = word_tokenize(tweet)\n",
    "    \n",
    "    for token, tag in pos_tag(tokens):\n",
    "                \n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "            \n",
    "            # remove tokens containing numbers\n",
    "    if numbers:\n",
    "        numbers = [str(i) for i in range(10)]\n",
    "        kill_list = []\n",
    "        for number in numbers:\n",
    "            kill_list = kill_list + [w for w in cleaned_tokens if number in w]\n",
    "        # removing selected tokens\n",
    "        cleaned_tokens = [w for w in cleaned_tokens if not w in kill_list]\n",
    "        \n",
    "        # merge tokens\n",
    "        merged = ' '\n",
    "        merged = merged.join(cleaned_tokens)\n",
    "        \n",
    "    return merged\n",
    "\n",
    "def clean(array):\n",
    "    for i, phrase in enumerate(array):\n",
    "        array[i] = clean_single(phrase)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening dataset files, cleaning and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raw_sentiment = pd.read_csv('data/training.1600000.processed.noemoticon.csv', \n",
    "#                           engine='python', \n",
    "#                           header=None, \n",
    "#                           names=['score', 'tweet_id', 'date', '?', 'account', 'text'])\n",
    "# df_sentiment = df_raw_sentiment[['score', 'text']]\n",
    "\n",
    "\n",
    "# classification = np.ones(len(df_sentiment), dtype=np.int)\n",
    "# classification[df_sentiment['score']==0] = -1\n",
    "\n",
    "# df_sentiment.insert(loc=2, column='classification', value=classification)\n",
    "# df_sentiment = df_sentiment[['classification', 'text']]\n",
    "\n",
    "# sentiment_classification = df_sentiment.classification.values\n",
    "# sentiment_text = clean(df_sentiment.text.values)\n",
    "\n",
    "# print(sentiment_classification.shape, sentiment_text.shape)\n",
    "\n",
    "# temp = pd.DataFrame({'text':sentiment_text,'classification':sentiment_classification})\n",
    "# temp.to_csv('data/sentiment_cleaned.csv', index=False)\n",
    "\n",
    "# df_raw_airlines = pd.read_csv('data/Tweets.csv', engine='python')\n",
    "# df_airlines = df_raw_airlines[['airline_sentiment', 'text']]\n",
    "\n",
    "# classification = np.zeros(len(df_airlines), dtype=np.int)\n",
    "# classification[df_airlines['airline_sentiment']=='negative'] = -1\n",
    "# classification[df_airlines['airline_sentiment']=='neutral'] = 0\n",
    "# classification[df_airlines['airline_sentiment']=='positive'] = 1\n",
    "\n",
    "# df_airlines.insert(loc=2, column='classification', value=classification)\n",
    "# df_airlines = df_airlines[['classification', 'text']]\n",
    "\n",
    "# airlines_classification = df_airlines.classification.values\n",
    "# airlines_text = clean(df_airlines.text.values)\n",
    "\n",
    "# print(airlines_classification.shape, airlines_text.shape)\n",
    "\n",
    "# temp = pd.DataFrame({'text':airlines_text,'classification':airlines_classification})\n",
    "# temp.to_csv('data/airlines_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse(path):\n",
    "#     g = gzip.open(path, 'r')\n",
    "#     for l in g:\n",
    "#         yield eval(l)\n",
    "\n",
    "# reviews = []\n",
    "# scores = []\n",
    "# for review in parse('data/reviews_Movies_and_TV_5.json.gz'):\n",
    "#     reviews.append(clean_single(review['reviewText']))\n",
    "#     scores.append(review['overall'])\n",
    "#     if len(scores) in [k*100000 for k in range(1,17)]:\n",
    "#         print(len(scores))\n",
    "# scores=np.asarray(scores).astype(dtype=np.int)\n",
    "# classification = np.zeros(scores.shape, dtype=np.int)\n",
    "# classification[scores<=2] = -1\n",
    "# classification[scores>=4] = 1\n",
    "# temp = pd.DataFrame({'text':reviews, 'classification':classification})\n",
    "# temp.to_csv('data/amazon_movies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opening cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airlines = pd.read_csv('data/airlines_cleaned.csv').dropna()\n",
    "df_airlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment = pd.read_csv('data/sentiment_cleaned.csv').dropna()\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon = pd.read_csv('data/amazon_movies.csv').dropna()\n",
    "df_amazon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment repartition over the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1,3,1)\n",
    "labels = ['positive', 'neutral', 'negative']\n",
    "sizes = [np.sum(df_airlines.classification==1), np.sum(df_airlines.classification==0), np.sum(df_airlines.classification==-1)]\n",
    "colors = ['green', 'yellow', 'red']\n",
    "patches, texts = plt.pie(sizes, colors=colors, shadow=True, startangle=140)\n",
    "plt.legend(patches, labels, loc=\"best\")\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.title('Sentiment distribution on airlines dataset')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "labels = ['positive', 'negative']\n",
    "sizes = [np.sum(df_sentiment.classification==1), np.sum(df_sentiment.classification==-1)]\n",
    "colors = ['green', 'red']\n",
    "patches, texts = plt.pie(sizes, colors=colors, shadow=True, startangle=90)\n",
    "plt.legend(patches, labels, loc=\"best\")\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.title('Sentiment distribution on sentiment dataset')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "labels = ['positive', 'neutral', 'negative']\n",
    "sizes = [np.sum(df_amazon.classification==1), np.sum(df_amazon.classification==0), np.sum(df_amazon.classification==-1)]\n",
    "colors = ['green', 'yellow', 'red']\n",
    "patches, texts = plt.pie(sizes, colors=colors, shadow=True, startangle=140)\n",
    "plt.legend(patches, labels, loc=\"best\")\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.title('Sentiment distribution on the dataset of amazon reviews')\n",
    "\n",
    "plt.savefig('figures/pie_charts_sentiment_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordclouds for the airlines dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "pos_text = \" \".join(df_airlines[df_airlines.classification==1].text.values)\n",
    "neg_text = \" \".join(df_airlines[df_airlines.classification==-1].text.values)\n",
    "neu_text = \" \".join(df_airlines[df_airlines.classification==0].text.values)\n",
    "\n",
    "wordcloud_pos = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(pos_text)\n",
    "wordcloud_neg = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(neg_text)\n",
    "wordcloud_neu = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(neu_text)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(wordcloud_pos, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title('Positive reviews')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(wordcloud_neu, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title('Neutral reviews')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(wordcloud_neg, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title('Negative reviews')\n",
    "plt.savefig('figures/airline_wordclouds.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordclouds for the sentiment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_text = \" \".join(df_sentiment[df_sentiment.classification==1].text.values)\n",
    "neg_text = \" \".join(df_sentiment[df_sentiment.classification==-1].text.values)\n",
    "\n",
    "wordcloud_pos = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(pos_text)\n",
    "wordcloud_neg = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(neg_text)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(wordcloud_pos, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title('Positive reviews')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(wordcloud_neg, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title('Neutral reviews')\n",
    "plt.savefig('figures/sentiment_wordclouds.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordclouds for the amazon reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_text = \" \".join(df_amazon[df_amazon.classification==1].text.values)\n",
    "neg_text = \" \".join(df_amazon[df_amazon.classification==-1].text.values)\n",
    "neu_text = \" \".join(df_amazon[df_amazon.classification==0].text.values)\n",
    "\n",
    "wordcloud_pos = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(pos_text)\n",
    "wordcloud_neg = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(neg_text)\n",
    "wordcloud_neu = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(neu_text)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(wordcloud_pos, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title('Positive reviews')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(wordcloud_neu, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title('Neutral reviews')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(wordcloud_neg, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title('Negative reviews')\n",
    "plt.savefig('figures/amazon_wordclouds.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TF-IDF Vectorizer without bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=10,\n",
    "                             norm='l2',\n",
    "                             ngram_range=(1,1))\n",
    "\n",
    "x_train = vectorizer.fit_transform(df_airlines.text.values).toarray()\n",
    "y_train = df_airlines.classification.values\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer with bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_bow = TfidfVectorizer(min_df=10,\n",
    "                                 norm='l2',\n",
    "                                 ngram_range=(1,3))\n",
    "\n",
    "x_train_bow = vectorizer_bow.fit_transform(df_airlines.text.values).toarray()\n",
    "x_train_bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train), total=4):\n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(x_train[train_index], y_train[train_index])\n",
    "    scores.append(classifier.score(x_train[test_index], y_train[test_index]))\n",
    "    \n",
    "np.mean(np.asarray(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train_bow), total=4):\n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(x_train_bow[train_index], y_train[train_index])\n",
    "    scores.append(classifier.score(x_train_bow[test_index], y_train[test_index]))\n",
    "    \n",
    "np.mean(np.asarray(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train), total=4):\n",
    "    classifier = SVC(kernel='rbf')\n",
    "    classifier.fit(x_train[train_index], y_train[train_index])\n",
    "    scores.append(classifier.score(x_train[test_index], y_train[test_index]))\n",
    "    \n",
    "np.mean(np.asarray(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train_bow), total=4):\n",
    "    classifier = SVC(kernel='rbf')\n",
    "    classifier.fit(x_train_bow[train_index], y_train[train_index])\n",
    "    scores.append(classifier.score(x_train_bow[test_index], y_train[test_index]))\n",
    "    \n",
    "np.mean(np.asarray(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "tmp = time()\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train), total=4):\n",
    "    classifier = RandomForestRegressor(n_estimators=50)\n",
    "    classifier.fit(x_train[train_index], y_train[train_index])\n",
    "    scores.append(classifier.score(x_train[test_index], y_train[test_index]))\n",
    "    \n",
    "np.mean(np.asarray(scores)), time()-tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = time()\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train_bow), total=4):\n",
    "    classifier = RandomForestRegressor(n_estimators=50)\n",
    "    classifier.fit(x_train_bow[train_index], y_train[train_index])\n",
    "    scores.append(classifier.score(x_train_bow[test_index], y_train[test_index]))\n",
    "    \n",
    "np.mean(np.asarray(scores)), time()-tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train), total=4):\n",
    "    classifier = AdaBoostClassifier(n_estimators=50, learning_rate=1.)\n",
    "    classifier.fit(x_train[train_index], y_train[train_index])\n",
    "    scores.append(classifier.score(x_train[test_index], y_train[test_index]))\n",
    "    \n",
    "np.mean(np.asarray(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train_bow), total=4):\n",
    "    classifier = AdaBoostClassifier(n_estimators=50, learning_rate=1.)\n",
    "    classifier.fit(x_train_bow[train_index], y_train[train_index])\n",
    "    scores.append(classifier.score(x_train_bow[test_index], y_train[test_index]))\n",
    "    \n",
    "np.mean(np.asarray(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully connected network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to one_hot\n",
    "def one_hot(x):\n",
    "    classes = np.asarray([-1, 0, 1])\n",
    "    array = np.zeros((*x.shape, classes.shape[0]), dtype=np.int)\n",
    "    for i, classe in enumerate(classes):\n",
    "        vector = np.zeros((1,classes.shape[0]), dtype=np.int)\n",
    "        vector[:,i]=1\n",
    "        array[x==classe] = vector\n",
    "    return classes, array\n",
    "\n",
    "classes, y_train_oh = one_hot(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, InputLayer, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy, Precision, Recall, AUC\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def FCN(x_train, y_train):\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(x_train.shape[1])))\n",
    "    model.add(Dense(units=32,\n",
    "                    activation='elu',\n",
    "                    kernel_initializer='lecun_normal'))\n",
    "    model.add(Dropout(rate=0.4))\n",
    "    \n",
    "    model.add(Dense(units=16,\n",
    "                    activation='elu',\n",
    "                    kernel_initializer='lecun_normal'))\n",
    "    model.add(Dropout(rate=0.4))\n",
    "    \n",
    "    model.add(Dense(units=y_train.shape[1], \n",
    "                    activation='softmax', \n",
    "                    kernel_initializer='lecun_normal'))\n",
    "    \n",
    "    METRICS = [CategoricalAccuracy(name='accuracy'),\n",
    "               Precision(name='precision'),\n",
    "               Recall(name='recall'),\n",
    "               AUC(name='auc')]\n",
    "\n",
    "    optimizer = Adam(lr=1e-5)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=METRICS)\n",
    "    model.summary()\n",
    "    \n",
    "    earlystop = EarlyStopping(monitor='val_loss',\n",
    "                             patience=15,\n",
    "                             restore_best_weights=True)\n",
    "    reduceLR = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                factor=np.sqrt(1e-1),\n",
    "                                verbose=1,\n",
    "                                patience=5)\n",
    "    history = model.fit(x=x_train,\n",
    "                        y=y_train,\n",
    "                        batch_size=32,\n",
    "                        epochs=10000,\n",
    "                        verbose=0,\n",
    "                        callbacks=[earlystop, reduceLR],\n",
    "                        validation_split=0.3,\n",
    "                        shuffle=True,\n",
    "                        workers=2)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train_bow), total=4):\n",
    "    model = FCN(x_train[train_index], y_train_oh[train_index])\n",
    "    scores.append(model.evaluate(x_train[test_index], y_train_oh[test_index]))\n",
    "                \n",
    "print(np.mean(np.asarray(scores), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train_bow), total=4):\n",
    "    model = FCN(x_train_bow[train_index], y_train_oh[train_index])\n",
    "    scores.append(model.evaluate(x_train_bow[test_index], y_train_oh[test_index]))\n",
    "    \n",
    "print(np.mean(np.asarray(scores), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_features = 2048 # around number of unigrams in the data\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(df_airlines.text.values)\n",
    "\n",
    "x_train_seq = tokenizer.texts_to_sequences(df_airlines.text.values)\n",
    "x_train_seq = pad_sequences(x_train_seq)\n",
    "\n",
    "print(x_train_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, LSTM, Embedding, SpatialDropout1D\n",
    "\n",
    "\n",
    "def RNN(x_train, y_train):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, 256, embeddings_initializer='lecun_normal'))\n",
    "    model.add(SpatialDropout1D(rate=0.4))\n",
    "    model.add(Bidirectional(LSTM(units=64, dropout=0.4, recurrent_dropout=0.4)))\n",
    "    \n",
    "    model.add(Dense(units=y_train.shape[1], \n",
    "                    activation='softmax', \n",
    "                    kernel_initializer='lecun_normal'))\n",
    "    \n",
    "    METRICS = [CategoricalAccuracy(name='accuracy'),\n",
    "               Precision(name='precision'),\n",
    "               Recall(name='recall'),\n",
    "               AUC(name='auc')]\n",
    "\n",
    "    optimizer = Adam(lr=1e-5)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=METRICS)\n",
    "    model.summary()\n",
    "    \n",
    "    earlystop = EarlyStopping(monitor='val_loss',\n",
    "                             patience=15,\n",
    "                             restore_best_weights=True)\n",
    "    reduceLR = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                factor=np.sqrt(1e-1),\n",
    "                                verbose=1,\n",
    "                                patience=5)\n",
    "    history = model.fit(x=x_train,\n",
    "                        y=y_train,\n",
    "                        batch_size=32,\n",
    "                        epochs=10000,\n",
    "                        verbose=2,\n",
    "                        callbacks=[earlystop, reduceLR],\n",
    "                        validation_split=0.3,\n",
    "                        shuffle=True,\n",
    "                        workers=3)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Macro parameters exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4, shuffle=True) \n",
    "all_scores = []\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train_seq), total=4):\n",
    "    model = RNN(x_train_seq[train_index], y_train_oh[train_index])\n",
    "    scores.append(model.evaluate(x_train_seq[test_index], y_train_oh[test_index]))\n",
    "\n",
    "all_scores.append(np.mean(np.asarray(scores), axis=0))\n",
    "        \n",
    "print(all_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of comparison of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    return [accuracy_score(y_true, y_pred), precision_score(y_true, y_pred),recall_score(y_true, y_pred),roc_auc_score(y_true, y_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=10,\n",
    "                             norm='l2',\n",
    "                             ngram_range=(1,1))\n",
    "vectorizer_bow = TfidfVectorizer(min_df=10,\n",
    "                                 norm='l2',\n",
    "                                 ngram_range=(1,3))\n",
    "\n",
    "max_features = 2048 # around number of unigrams in the data\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(df_airlines.text.values)\n",
    "\n",
    "x_train = vectorizer.fit_transform(df_airlines.text.values).toarray()\n",
    "x_train_bow = vectorizer_bow.fit_transform(df_airlines.text.values).toarray()\n",
    "x_train_seq = pad_sequences(tokenizer.texts_to_sequences(df_airlines.text.values))\n",
    "y_train = df_airlines.classification.values\n",
    "y_train_oh = one_hot(y_train)\n",
    "\n",
    "x_sentiment = vectorizer.transform(df_sentiment.text.values).toarray()\n",
    "x_sentiment_bow = vectorizer_bow.transform(df_sentiment.text.values).toarray()\n",
    "y_sentiment_seq = pad_sequences(tokenizer.texts_to_sequences(df_sentiment.text.values))\n",
    "y_sentiment = df_sentiment.classification.values\n",
    "y_sentiment_oh = one_hot(y_sentiment)\n",
    "\n",
    "x_amazon = vectorizer.transform(df_amazon.text.values).toarray()\n",
    "x_amazon_bow = vectorizer_bow.transform(df_amazon.text.values).toarray()\n",
    "y_amazon_seq = pad_sequences(tokenizer.texts_to_sequences(df_amazon.text.values))\n",
    "y_amazon = df_amazon.classification.values\n",
    "y_amazon_oh = one_hot(y_amazon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_nb = []\n",
    "amazon_svm = []\n",
    "amazon_adaboost = []\n",
    "amazon_ann = []\n",
    "amazon_rnn = []\n",
    "\n",
    "sentiment_nb = []\n",
    "sentiment_svm = []\n",
    "sentiment_adaboost = []\n",
    "sentiment_ann = []\n",
    "sentiment_rnn = []\n",
    "\n",
    "airlines_nb = []\n",
    "airlines_svm = []\n",
    "airlines_adaboost = []\n",
    "airlines_ann = []\n",
    "airlines_rnn = []\n",
    "\n",
    "for k in range(5): #needs to average the results\n",
    "    for train_index, test_index in tqdm(kf.split(x_train), total=4):\n",
    "        # Naive Bayes\n",
    "        classifier = MultinomialNB()\n",
    "        classifier.fit(x_train[train_index], y_train[train_index])\n",
    "            \n",
    "        sentiment_nb.append(evaluate(y_sentiment, classifier.predict(x_sentiment)))\n",
    "        amazon_nb.append(evaluate(y_amazon, classifier.predict(x_amazon)))\n",
    "        airlines_nb.append(evaluate(y_train[test_index], classifier.predict(x_train[test_index])))\n",
    "        # SVM\n",
    "        classifier = SVC(kernel='rbf')\n",
    "        classifier.fit(x_train[train_index], y_train[train_index])\n",
    "            \n",
    "        sentiment_svm.append(evaluate(y_sentiment, classifier.predict(x_sentiment)))\n",
    "        amazon_svm.append(evaluate(y_amazon, classifier.predict(x_amazon)))\n",
    "        airlines_svm.append(evaluate(y_train[test_index], classifier.predict(x_train[test_index])))\n",
    "        # AdaBoost\n",
    "        classifier = AdaBoostClassifier(n_estimators=50, learning_rate=1.)\n",
    "        classifier.fit(x_train[train_index], y_train[train_index])\n",
    "            \n",
    "        sentiment_adaboost.append(evaluate(y_sentiment, classifier.predict(x_sentiment)))\n",
    "        amazon_adaboost.append(evaluate(y_amazon, classifier.predict(x_amazon)))\n",
    "        airlines_adaboost.append(evaluate(y_train[test_index], classifier.predict(x_train[test_index])))\n",
    "        # ANN\n",
    "        model = FCN(x_train[train_index], y_train_oh[train_index])\n",
    "        sentiment_ann.append(model.evaluate(x_sentiment, y_sentiment_oh)[1:])\n",
    "        amazon_ann.append(model.evaluate(x_amazon, y_amazon_oh)[1:])\n",
    "        airlines_ann.append(model.evaluate(x_train[test_index], y_train_oh[test_index])[1:])\n",
    "        # RNN\n",
    "        model = RNN(x_train_seq[train_index], y_train_oh[train_index])\n",
    "        sentiment_rnn.append(model.evaluate(x_sentiment_seq, y_sentiment_oh)[1:])\n",
    "        amazon_rnn.append(model.evaluate(x_amazon_seq, y_amazon_oh)[1:])\n",
    "        airlines_rnn.append(model.evaluate(x_train_seq[test_index], y_train_oh[test_index])[1:])\n",
    "            \n",
    "amazon = pd.DataFrame({'NB':amazon_nb, 'SVM':amazon_svm, 'AdaBoost':amazon_adaboost, 'ANN':amazon_ann, 'RNN':amazon_rnn})\n",
    "amazon.to_csv('amazon_scores.csv')\n",
    "\n",
    "sentiment = pd.DataFrame({'NB':sentiment_nb, 'SVM':sentiment_svm, 'AdaBoost':sentiment_adaboost, 'ANN':sentiment_ann, 'RNN':sentiment_rnn})\n",
    "sentiment.to_csv('sentiment_scores.csv')         \n",
    "\n",
    "airlines = pd.DataFrame({'NB':airlines_nb, 'SVM':airlines_svm, 'AdaBoost':airlines_adaboost, 'ANN':airlines_ann, 'RNN':airlines_rnn})\n",
    "airlines.to_csv('airlines_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_nb = []\n",
    "amazon_svm = []\n",
    "amazon_adaboost = []\n",
    "amazon_ann = []\n",
    "amazon_rnn = []\n",
    "\n",
    "sentiment_nb = []\n",
    "sentiment_svm = []\n",
    "sentiment_adaboost = []\n",
    "sentiment_ann = []\n",
    "sentiment_rnn = []\n",
    "\n",
    "airlines_nb = []\n",
    "airlines_svm = []\n",
    "airlines_adaboost = []\n",
    "airlines_ann = []\n",
    "airlines_rnn = []\n",
    "\n",
    "x_train = x_train_bow\n",
    "x_amazon = x_amazon_bow\n",
    "x_sentiment = x_sentiment_bow\n",
    "\n",
    "for k in range(5): #needs to average the results\n",
    "    for train_index, test_index in tqdm(kf.split(x_train), total=4):\n",
    "        # Naive Bayes\n",
    "        classifier = MultinomialNB()\n",
    "        classifier.fit(x_train[train_index], y_train[train_index])\n",
    "            \n",
    "        sentiment_nb.append(evaluate(y_sentiment, classifier.predict(x_sentiment)))\n",
    "        amazon_nb.append(evaluate(y_amazon, classifier.predict(x_amazon)))\n",
    "        airlines_nb.append(evaluate(y_train[test_index], classifier.predict(x_train[test_index])))\n",
    "        # SVM\n",
    "        classifier = SVC(kernel='rbf')\n",
    "        classifier.fit(x_train[train_index], y_train[train_index])\n",
    "            \n",
    "        sentiment_svm.append(evaluate(y_sentiment, classifier.predict(x_sentiment)))\n",
    "        amazon_svm.append(evaluate(y_amazon, classifier.predict(x_amazon)))\n",
    "        airlines_svm.append(evaluate(y_train[test_index], classifier.predict(x_train[test_index])))\n",
    "        # AdaBoost\n",
    "        classifier = AdaBoostClassifier(n_estimators=50, learning_rate=1.)\n",
    "        classifier.fit(x_train[train_index], y_train[train_index])\n",
    "            \n",
    "        sentiment_adaboost.append(evaluate(y_sentiment, classifier.predict(x_sentiment)))\n",
    "        amazon_adaboost.append(evaluate(y_amazon, classifier.predict(x_amazon)))\n",
    "        airlines_adaboost.append(evaluate(y_train[test_index], classifier.predict(x_train[test_index])))\n",
    "        # ANN\n",
    "        model = FCN(x_train[train_index], y_train_oh[train_index])\n",
    "        sentiment_ann.append(model.evaluate(x_sentiment, y_sentiment_oh)[1:])\n",
    "        amazon_ann.append(model.evaluate(x_amazon, y_amazon_oh)[1:])\n",
    "        airlines_ann.append(model.evaluate(x_train[test_index], y_train_oh[test_index])[1:])\n",
    "        # RNN\n",
    "        model = RNN(x_train_seq[train_index], y_train_oh[train_index])\n",
    "        sentiment_rnn.append(model.evaluate(x_sentiment_seq, y_sentiment_oh)[1:])\n",
    "        amazon_rnn.append(model.evaluate(x_amazon_seq, y_amazon_oh)[1:])\n",
    "        airlines_rnn.append(model.evaluate(x_train_seq[test_index], y_train_oh[test_index])[1:])\n",
    "            \n",
    "amazon = pd.DataFrame({'NB':amazon_nb, 'SVM':amazon_svm, 'AdaBoost':amazon_adaboost, 'ANN':amazon_ann, 'RNN':amazon_rnn})\n",
    "amazon.to_csv('amazon_scores_bow.csv')\n",
    "\n",
    "sentiment = pd.DataFrame({'NB':sentiment_nb, 'SVM':sentiment_svm, 'AdaBoost':sentiment_adaboost, 'ANN':sentiment_ann, 'RNN':sentiment_rnn})\n",
    "sentiment.to_csv('sentiment_scores_bow.csv')         \n",
    "\n",
    "airlines = pd.DataFrame({'NB':airlines_nb, 'SVM':airlines_svm, 'AdaBoost':airlines_adaboost, 'ANN':airlines_ann, 'RNN':airlines_rnn})\n",
    "airlines.to_csv('airlines_scores_bow.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "107px",
    "width": "241px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
