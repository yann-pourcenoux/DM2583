{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/yann/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/yann/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/yann/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/yann/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import gzip\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine learning librairies and tools\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Dropout\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Embedding, SpatialDropout1D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy, Precision, Recall, AUC\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re, string\n",
    "\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "def clean_single(tweet, stop_words = stopwords.words('english'), numbers=True):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "    # Removing links\n",
    "    tweet = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', tweet)\n",
    "    # Removing @\n",
    "    tweet = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", tweet)\n",
    "        \n",
    "    # Removing currencies\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    \n",
    "    tokens = word_tokenize(tweet)\n",
    "    \n",
    "    for token, tag in pos_tag(tokens):\n",
    "                \n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "            \n",
    "            # remove tokens containing numbers\n",
    "    if numbers:\n",
    "        numbers = [str(i) for i in range(10)]\n",
    "        kill_list = []\n",
    "        for number in numbers:\n",
    "            kill_list = kill_list + [w for w in cleaned_tokens if number in w]\n",
    "        # removing selected tokens\n",
    "        cleaned_tokens = [w for w in cleaned_tokens if not w in kill_list]\n",
    "        \n",
    "        # merge tokens\n",
    "        merged = ' '\n",
    "        merged = merged.join(cleaned_tokens)\n",
    "        \n",
    "    return merged\n",
    "\n",
    "def clean(array):\n",
    "    for i, phrase in enumerate(array):\n",
    "        array[i] = clean_single(phrase)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening dataset files, cleaning and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raw_sentiment = pd.read_csv('data/training.1600000.processed.noemoticon.csv', \n",
    "#                           engine='python', \n",
    "#                           header=None, \n",
    "#                           names=['score', 'tweet_id', 'date', '?', 'account', 'text'])\n",
    "# df_sentiment = df_raw_sentiment[['score', 'text']]\n",
    "\n",
    "\n",
    "# classification = np.ones(len(df_sentiment), dtype=np.int)\n",
    "# classification[df_sentiment['score']==0] = -1\n",
    "\n",
    "# df_sentiment.insert(loc=2, column='classification', value=classification)\n",
    "# df_sentiment = df_sentiment[['classification', 'text']]\n",
    "\n",
    "# sentiment_classification = df_sentiment.classification.values\n",
    "# sentiment_text = clean(df_sentiment.text.values)\n",
    "\n",
    "# print(sentiment_classification.shape, sentiment_text.shape)\n",
    "\n",
    "# temp = pd.DataFrame({'text':sentiment_text,'classification':sentiment_classification})\n",
    "# temp.to_csv('data/sentiment_cleaned.csv', index=False)\n",
    "\n",
    "# df_raw_airlines = pd.read_csv('data/Tweets.csv', engine='python')\n",
    "# df_airlines = df_raw_airlines[['airline_sentiment', 'text']]\n",
    "\n",
    "# classification = np.zeros(len(df_airlines), dtype=np.int)\n",
    "# classification[df_airlines['airline_sentiment']=='negative'] = -1\n",
    "# classification[df_airlines['airline_sentiment']=='neutral'] = 0\n",
    "# classification[df_airlines['airline_sentiment']=='positive'] = 1\n",
    "\n",
    "# df_airlines.insert(loc=2, column='classification', value=classification)\n",
    "# df_airlines = df_airlines[['classification', 'text']]\n",
    "\n",
    "# airlines_classification = df_airlines.classification.values\n",
    "# airlines_text = clean(df_airlines.text.values)\n",
    "\n",
    "# print(airlines_classification.shape, airlines_text.shape)\n",
    "\n",
    "# temp = pd.DataFrame({'text':airlines_text,'classification':airlines_classification})\n",
    "# temp.to_csv('data/airlines_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse(path):\n",
    "#     g = gzip.open(path, 'r')\n",
    "#     for l in g:\n",
    "#         yield eval(l)\n",
    "\n",
    "# reviews = []\n",
    "# scores = []\n",
    "# for review in parse('data/reviews_Movies_and_TV_5.json.gz'):\n",
    "#     reviews.append(clean_single(review['reviewText']))\n",
    "#     scores.append(review['overall'])\n",
    "#     if len(scores) in [k*100000 for k in range(1,17)]:\n",
    "#         print(len(scores))\n",
    "# scores=np.asarray(scores).astype(dtype=np.int)\n",
    "# classification = np.zeros(scores.shape, dtype=np.int)\n",
    "# classification[scores<=2] = -1\n",
    "# classification[scores>=4] = 1\n",
    "# temp = pd.DataFrame({'text':reviews, 'classification':classification})\n",
    "# temp.to_csv('data/amazon_movies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opening cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>say</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>plus 've add commercial experience ... tacky</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n't today ... must mean need take another trip</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'s really aggressive blast obnoxious `` entert...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'s really big bad thing</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  classification\n",
       "0                                                say               0\n",
       "1       plus 've add commercial experience ... tacky               1\n",
       "2     n't today ... must mean need take another trip               0\n",
       "3  's really aggressive blast obnoxious `` entert...              -1\n",
       "4                            's really big bad thing              -1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airlines = pd.read_csv('data/airlines_cleaned.csv').dropna()\n",
    "df_airlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>awww 's bummer shoulda get david carr third day</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>upset ca n't update facebook texting ... might...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dive many time ball managed save rest go bound</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'s behave 'm mad ca n't see</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  classification\n",
       "0    awww 's bummer shoulda get david carr third day              -1\n",
       "1  upset ca n't update facebook texting ... might...              -1\n",
       "2     dive many time ball managed save rest go bound              -1\n",
       "3                    whole body feel itchy like fire              -1\n",
       "4                        's behave 'm mad ca n't see              -1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentiment = pd.read_csv('data/sentiment_cleaned.csv').dropna()\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>charming version classic dicken 's tale henry ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>good emotionally move christmas carol dickens ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>n't get wrong winkler wonderful character acto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>henry winkler good twist classic story convent...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>one best scrooge movie henry winkler outdo cas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  \\\n",
       "0           0  charming version classic dicken 's tale henry ...   \n",
       "1           1  good emotionally move christmas carol dickens ...   \n",
       "2           2  n't get wrong winkler wonderful character acto...   \n",
       "3           3  henry winkler good twist classic story convent...   \n",
       "4           4  one best scrooge movie henry winkler outdo cas...   \n",
       "\n",
       "   classification  \n",
       "0               1  \n",
       "1               0  \n",
       "2               0  \n",
       "3               1  \n",
       "4               1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_amazon = pd.read_csv('data/amazon_movies.csv').dropna()\n",
    "df_amazon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment repartition over the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1,3,1)\n",
    "labels = ['positive', 'neutral', 'negative']\n",
    "sizes = [np.sum(df_airlines.classification==1), np.sum(df_airlines.classification==0), np.sum(df_airlines.classification==-1)]\n",
    "colors = ['green', 'yellow', 'red']\n",
    "patches, texts = plt.pie(sizes, colors=colors, shadow=True, startangle=140)\n",
    "plt.legend(patches, labels, loc=\"best\")\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.title('Sentiment distribution on airlines dataset')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "labels = ['positive', 'negative']\n",
    "sizes = [np.sum(df_sentiment.classification==1), np.sum(df_sentiment.classification==-1)]\n",
    "colors = ['green', 'red']\n",
    "patches, texts = plt.pie(sizes, colors=colors, shadow=True, startangle=90)\n",
    "plt.legend(patches, labels, loc=\"best\")\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.title('Sentiment distribution on sentiment dataset')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "labels = ['positive', 'neutral', 'negative']\n",
    "sizes = [np.sum(df_amazon.classification==1), np.sum(df_amazon.classification==0), np.sum(df_amazon.classification==-1)]\n",
    "colors = ['green', 'yellow', 'red']\n",
    "patches, texts = plt.pie(sizes, colors=colors, shadow=True, startangle=140)\n",
    "plt.legend(patches, labels, loc=\"best\")\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.title('Sentiment distribution on the dataset of amazon reviews')\n",
    "\n",
    "plt.savefig('figures/pie_charts_sentiment_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordclouds for the airlines dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_text = \" \".join(df_airlines[df_airlines.classification==1].text.values)\n",
    "neg_text = \" \".join(df_airlines[df_airlines.classification==-1].text.values)\n",
    "neu_text = \" \".join(df_airlines[df_airlines.classification==0].text.values)\n",
    "\n",
    "wordcloud_pos = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(pos_text)\n",
    "wordcloud_neg = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(neg_text)\n",
    "wordcloud_neu = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(neu_text)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(wordcloud_pos, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title('Positive reviews')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(wordcloud_neu, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title('Neutral reviews')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(wordcloud_neg, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title('Negative reviews')\n",
    "plt.savefig('figures/airline_wordclouds.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordclouds for the sentiment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_text = \" \".join(df_sentiment[df_sentiment.classification==1].text.values)\n",
    "neg_text = \" \".join(df_sentiment[df_sentiment.classification==-1].text.values)\n",
    "\n",
    "wordcloud_pos = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(pos_text)\n",
    "wordcloud_neg = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(neg_text)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(wordcloud_pos, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title('Positive reviews')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(wordcloud_neg, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title('Neutral reviews')\n",
    "plt.savefig('figures/sentiment_wordclouds.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordclouds for the amazon reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_text = \" \".join(df_amazon[df_amazon.classification==1].text.values)\n",
    "neg_text = \" \".join(df_amazon[df_amazon.classification==-1].text.values)\n",
    "neu_text = \" \".join(df_amazon[df_amazon.classification==0].text.values)\n",
    "\n",
    "wordcloud_pos = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(pos_text)\n",
    "wordcloud_neg = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(neg_text)\n",
    "wordcloud_neu = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(neu_text)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(wordcloud_pos, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title('Positive reviews')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(wordcloud_neu, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title('Neutral reviews')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(wordcloud_neg, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title('Negative reviews')\n",
    "plt.savefig('figures/amazon_wordclouds.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TF-IDF Vectorizer without bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14604, 1522)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=10,\n",
    "                             norm='l2',\n",
    "                             ngram_range=(1,1))\n",
    "\n",
    "x_train = vectorizer.fit_transform(df_airlines.text.values).toarray()\n",
    "y_train = df_airlines.classification.values\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer with bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14604, 2290)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_bow = TfidfVectorizer(min_df=10,\n",
    "                                 norm='l2',\n",
    "                                 ngram_range=(1,3))\n",
    "\n",
    "x_train_bow = vectorizer_bow.fit_transform(df_airlines.text.values).toarray()\n",
    "x_train_bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train), total=4):\n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(x_train[train_index], y_train[train_index])\n",
    "    scores.append(classifier.score(x_train[test_index], y_train[test_index]))\n",
    "    \n",
    "np.mean(np.asarray(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train_bow), total=4):\n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(x_train_bow[train_index], y_train[train_index])\n",
    "    scores.append(classifier.score(x_train_bow[test_index], y_train[test_index]))\n",
    "    \n",
    "np.mean(np.asarray(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train), total=4):\n",
    "    classifier = SVC(kernel='rbf')\n",
    "    classifier.fit(x_train[train_index], y_train[train_index])\n",
    "    scores.append(classifier.score(x_train[test_index], y_train[test_index]))\n",
    "    \n",
    "np.mean(np.asarray(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train_bow), total=4):\n",
    "    classifier = SVC(kernel='rbf')\n",
    "    classifier.fit(x_train_bow[train_index], y_train[train_index])\n",
    "    scores.append(classifier.score(x_train_bow[test_index], y_train[test_index]))\n",
    "    \n",
    "np.mean(np.asarray(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train), total=4):\n",
    "    classifier = RandomForestRegressor(n_estimators=50)\n",
    "    classifier.fit(x_train[train_index], y_train[train_index])\n",
    "    scores.append(classifier.score(x_train[test_index], y_train[test_index]))\n",
    "    \n",
    "np.mean(np.asarray(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train_bow), total=4):\n",
    "    classifier = RandomForestRegressor(n_estimators=50)\n",
    "    classifier.fit(x_train_bow[train_index], y_train[train_index])\n",
    "    scores.append(classifier.score(x_train_bow[test_index], y_train[test_index]))\n",
    "    \n",
    "np.mean(np.asarray(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train), total=4):\n",
    "    classifier = AdaBoostClassifier(n_estimators=50, learning_rate=1.)\n",
    "    classifier.fit(x_train[train_index], y_train[train_index])\n",
    "    scores.append(classifier.score(x_train[test_index], y_train[test_index]))\n",
    "    \n",
    "np.mean(np.asarray(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train_bow), total=4):\n",
    "    classifier = AdaBoostClassifier(n_estimators=50, learning_rate=1.)\n",
    "    classifier.fit(x_train_bow[train_index], y_train[train_index])\n",
    "    scores.append(classifier.score(x_train_bow[test_index], y_train[test_index]))\n",
    "    \n",
    "np.mean(np.asarray(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully connected network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to one_hot\n",
    "def one_hot(x):\n",
    "    classes = np.asarray([-1, 0, 1])\n",
    "    array = np.zeros((*x.shape, classes.shape[0]), dtype=np.int)\n",
    "    for i, classe in enumerate(classes):\n",
    "        vector = np.zeros((1,classes.shape[0]), dtype=np.int)\n",
    "        vector[:,i]=1\n",
    "        array[x==classe] = vector\n",
    "    return array\n",
    "\n",
    "y_train_oh = one_hot(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FCN(x_train, y_train):\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(x_train.shape[1])))\n",
    "    model.add(Dense(units=32,\n",
    "                    activation='elu',\n",
    "                    kernel_initializer='lecun_normal'))\n",
    "    model.add(Dropout(rate=0.4))\n",
    "    \n",
    "    model.add(Dense(units=16,\n",
    "                    activation='elu',\n",
    "                    kernel_initializer='lecun_normal'))\n",
    "    model.add(Dropout(rate=0.4))\n",
    "    \n",
    "    model.add(Dense(units=y_train.shape[1], \n",
    "                    activation='softmax', \n",
    "                    kernel_initializer='lecun_normal'))\n",
    "    \n",
    "    METRICS = [CategoricalAccuracy(name='accuracy'),\n",
    "               Precision(name='precision'),\n",
    "               Recall(name='recall'),\n",
    "               AUC(name='auc')]\n",
    "\n",
    "    optimizer = Adam(lr=1e-5)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=METRICS)\n",
    "    model.summary()\n",
    "    \n",
    "    earlystop = EarlyStopping(monitor='val_loss',\n",
    "                             patience=15,\n",
    "                             restore_best_weights=True)\n",
    "    reduceLR = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                factor=np.sqrt(1e-1),\n",
    "                                verbose=1,\n",
    "                                patience=5)\n",
    "    history = model.fit(x=x_train,\n",
    "                        y=y_train,\n",
    "                        batch_size=32,\n",
    "                        epochs=10000,\n",
    "                        verbose=0,\n",
    "                        callbacks=[earlystop, reduceLR],\n",
    "                        validation_split=0.3,\n",
    "                        shuffle=True,\n",
    "                        workers=4)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train_bow), total=4):\n",
    "    model = FCN(x_train[train_index], y_train_oh[train_index])\n",
    "    scores.append(model.evaluate(x_train[test_index], y_train_oh[test_index]))\n",
    "                \n",
    "print(np.mean(np.asarray(scores), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train_bow), total=4):\n",
    "    model = FCN(x_train_bow[train_index], y_train_oh[train_index])\n",
    "    scores.append(model.evaluate(x_train_bow[test_index], y_train_oh[test_index]))\n",
    "    \n",
    "print(np.mean(np.asarray(scores), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14604, 21)\n"
     ]
    }
   ],
   "source": [
    "max_features = 2048 # around number of unigrams in the data\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(df_airlines.text.values)\n",
    "\n",
    "x_train_seq = tokenizer.texts_to_sequences(df_airlines.text.values)\n",
    "x_train_seq = pad_sequences(x_train_seq)\n",
    "\n",
    "print(x_train_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(x_train, y_train):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, 256, embeddings_initializer='lecun_normal'))\n",
    "    model.add(SpatialDropout1D(rate=0.4))\n",
    "    model.add(Bidirectional(LSTM(units=64, dropout=0.4, recurrent_dropout=0.4)))\n",
    "    \n",
    "    model.add(Dense(units=y_train.shape[1], \n",
    "                    activation='softmax', \n",
    "                    kernel_initializer='lecun_normal'))\n",
    "    \n",
    "    METRICS = [CategoricalAccuracy(name='accuracy'),\n",
    "               Precision(name='precision'),\n",
    "               Recall(name='recall'),\n",
    "               AUC(name='auc')]\n",
    "\n",
    "    optimizer = Adam(lr=1e-5)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=METRICS)\n",
    "    model.summary()\n",
    "    \n",
    "    earlystop = EarlyStopping(monitor='val_loss',\n",
    "                             patience=15,\n",
    "                             restore_best_weights=True)\n",
    "    reduceLR = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                factor=np.sqrt(1e-1),\n",
    "                                verbose=1,\n",
    "                                patience=5)\n",
    "    history = model.fit(x=x_train,\n",
    "                        y=y_train,\n",
    "                        batch_size=32,\n",
    "                        epochs=10000,\n",
    "                        verbose=0,\n",
    "                        callbacks=[earlystop, reduceLR],\n",
    "                        validation_split=0.3,\n",
    "                        shuffle=True,\n",
    "                        workers=4)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Macro parameters exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4, shuffle=True) \n",
    "all_scores = []\n",
    "scores = []\n",
    "for train_index, test_index in tqdm(kf.split(x_train_seq), total=4):\n",
    "    model = RNN(x_train_seq[train_index], y_train_oh[train_index])\n",
    "    scores.append(model.evaluate(x_train_seq[test_index], y_train_oh[test_index]))\n",
    "\n",
    "all_scores.append(np.mean(np.asarray(scores), axis=0))\n",
    "        \n",
    "print(all_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of comparison of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt(y):\n",
    "    array = np.zeros(y.shape, dtype=np.int)\n",
    "    for i, row in enumerate(y):\n",
    "        array[i][np.argmax(row)]=1\n",
    "    return array\n",
    "\n",
    "def evaluate(dataframe, classifier, preprocessing_x, preprocessing_y=None, NN=False):\n",
    "    size = len(dataframe)\n",
    "    scores = []\n",
    "    n = size//100000\n",
    "    for k in range(n):\n",
    "        y_true = dataframe.classification.values[k*100000:(k+1)*100000]\n",
    "        if preprocessing_y:\n",
    "            y_true = preprocessing_y(y_true)\n",
    "            \n",
    "        if NN:\n",
    "            scores.append(model.evaluate(preprocessing_x(dataframe.text.values[k*100000:(k+1)*100000]), y_true, verbose=0)[1:])\n",
    "        \n",
    "        else:\n",
    "            y_pred = classifier.predict(preprocessing_x(dataframe.text.values[k*100000:(k+1)*100000]))\n",
    "            scores.append([accuracy_score(y_true, y_pred), \n",
    "                           precision_score(y_true, y_pred, average='weighted'),\n",
    "                           recall_score(y_true, y_pred, average='weighted', zero_division=0)])\n",
    "        \n",
    "    \n",
    "    \n",
    "    # last values\n",
    "    y_true = dataframe.classification.values[(k+1)*100000:]\n",
    "    if preprocessing_y:\n",
    "        y_true = preprocessing_y(y_true)\n",
    "    \n",
    "    if NN:\n",
    "        scores.append(model.evaluate(preprocessing_x(dataframe.text.values[(k+1)*100000:]), y_true, verbose=0)[1:])\n",
    "        \n",
    "    else:\n",
    "        y_pred = classifier.predict(preprocessing_x(dataframe.text.values[(k+1)*100000:]))\n",
    "        scores.append([accuracy_score(y_true, y_pred), \n",
    "                       precision_score(y_true, y_pred, average='weighted'),\n",
    "                       recall_score(y_true, y_pred, average='weighted', zero_division=0)])\n",
    "        \n",
    "    return np.mean(np.asarray(scores), axis=0)\n",
    "\n",
    "def get_scores(y_true, y_pred):\n",
    "    return [accuracy_score(y_true, y_pred), precision_score(y_true, y_pred, average='weighted'),recall_score(y_true, y_pred, average='weighted')]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=10,\n",
    "                             norm='l2',\n",
    "                             ngram_range=(1,1))\n",
    "vectorizer_bow = TfidfVectorizer(min_df=10,\n",
    "                                 norm='l2',\n",
    "                                 ngram_range=(1,3))\n",
    "\n",
    "max_features = 2048 # around number of unigrams in the data\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(df_airlines.text.values)\n",
    "\n",
    "x_train = vectorizer.fit_transform(df_airlines.text.values).toarray()\n",
    "x_train_bow = vectorizer_bow.fit_transform(df_airlines.text.values).toarray()\n",
    "x_train_seq = pad_sequences(tokenizer.texts_to_sequences(df_airlines.text.values))\n",
    "y_train = df_airlines.classification.values\n",
    "y_train_oh = one_hot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ca58f64e94447fb78c7ed0c936a240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "evaluation\n",
      "AdaBoost\n",
      "evaluation sentiment\n",
      "evaluation amazon\n",
      "evaluation airlines\n",
      "ANN\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 32)                48736     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 49,315\n",
      "Trainable params: 49,315\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch 00199: ReduceLROnPlateau reducing learning rate to 3.1622775802825263e-06.\n",
      "\n",
      "Epoch 00204: ReduceLROnPlateau reducing learning rate to 9.99999983430526e-07.\n",
      "\n",
      "Epoch 00209: ReduceLROnPlateau reducing learning rate to 3.162277652184396e-07.\n",
      "evaluation sentiment\n"
     ]
    }
   ],
   "source": [
    "amazon_nb = []\n",
    "amazon_svm = []\n",
    "amazon_adaboost = []\n",
    "amazon_ann = []\n",
    "amazon_rnn = []\n",
    "\n",
    "sentiment_nb = []\n",
    "sentiment_svm = []\n",
    "sentiment_adaboost = []\n",
    "sentiment_ann = []\n",
    "sentiment_rnn = []\n",
    "\n",
    "airlines_nb = []\n",
    "airlines_svm = []\n",
    "airlines_adaboost = []\n",
    "airlines_ann = []\n",
    "airlines_rnn = []\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "for k in range(5): #needs to average the results\n",
    "    for train_index, test_index in tqdm(kf.split(x_train), total=4):\n",
    "        indexes_amazon = np.random.choice(a=len(df_amazon), size=len(df_amazon), replace=False)\n",
    "        indexes_sentiment = np.random.choice(a=len(df_sentiment), size=len(df_sentiment), replace=False)\n",
    "        \n",
    "        print('Naive Bayes')\n",
    "        # Naive Bayes\n",
    "        classifier = MultinomialNB()\n",
    "        classifier.fit(x_train[train_index], y_train[train_index])\n",
    "        \n",
    "        print('evaluation')\n",
    "        amazon_nb.append(evaluate(df_amazon.iloc[indexes_amazon], classifier, lambda x:vectorizer.transform(x).toarray()))\n",
    "        sentiment_nb.append(evaluate(df_sentiment.iloc[indexes_sentiment], classifier, lambda x:vectorizer.transform(x).toarray()))\n",
    "        airlines_nb.append(get_scores(y_train[test_index], classifier.predict(x_train[test_index])))\n",
    "        \n",
    "#         print('SVM')\n",
    "#         # SVM\n",
    "#         classifier = SVC(kernel='rbf')\n",
    "#         classifier.fit(x_train[train_index], y_train[train_index])\n",
    "        \n",
    "#         print('evaluation sentiment')\n",
    "#         sentiment_svm.append(evaluate(df_sentiment.iloc[indexes_sentiment], classifier, lambda x:vectorizer.transform(x).toarray()))\n",
    "#         print('evaluation amazon')\n",
    "#         amazon_svm.append(evaluate(df_amazon.iloc[indexes_amazon], classifier, lambda x:vectorizer.transform(x).toarray()))\n",
    "#         print('evaluation airlines')\n",
    "#         airlines_svm.append(get_scores(y_train[test_index], classifier.predict(x_train[test_index])))\n",
    "        \n",
    "        print('AdaBoost')\n",
    "        # AdaBoost\n",
    "        classifier = AdaBoostClassifier(n_estimators=50, learning_rate=1.)\n",
    "        classifier.fit(x_train[train_index], y_train[train_index])\n",
    "        \n",
    "        print('evaluation sentiment')\n",
    "        sentiment_adaboost.append(evaluate(df_sentiment.iloc[indexes_sentiment], classifier, lambda x:vectorizer.transform(x).toarray()))\n",
    "        print('evaluation amazon')\n",
    "        amazon_adaboost.append(evaluate(df_amazon.iloc[indexes_amazon], classifier, lambda x:vectorizer.transform(x).toarray()))\n",
    "        print('evaluation airlines')\n",
    "        airlines_adaboost.append(get_scores(y_train[test_index], classifier.predict(x_train[test_index])))\n",
    "        \n",
    "        print('ANN')\n",
    "        # ANN\n",
    "        model = FCN(x_train[train_index], y_train_oh[train_index])\n",
    "                \n",
    "        print('evaluation sentiment')\n",
    "        sentiment_ann.append(evaluate(df_sentiment.iloc[indexes_sentiment], model, lambda x:vectorizer.transform(x).toarray(), one_hot, NN=True))\n",
    "        print('evaluation amazon')\n",
    "        amazon_ann.append(evaluate(df_amazon.iloc[indexes_amazon], model, lambda x:vectorizer.transform(x).toarray(), one_hot, NN=True))\n",
    "        print('evaluation airlines')\n",
    "        airlines_ann.append(model.evaluate(x_train[test_index], y_train_oh[test_index])[1:])\n",
    "        \n",
    "        print('RNN')\n",
    "        # RNN\n",
    "        model = RNN(x_train_seq[train_index], y_train_oh[train_index])\n",
    "                \n",
    "        print('evaluation sentiment')\n",
    "        sentiment_rnn.append(model.evaluate(df_sentiment.iloc[indexes_sentiment], model, lambda x:pad_sequences(tokenizer.texts_to_sequences(x)), one_hot, NN=True))\n",
    "        print('evaluation amazon')\n",
    "        amazon_rnn.append(model.evaluate(df_amazon.iloc[indexes_amazon], model, lambda x:pad_sequences(tokenizer.texts_to_sequences(x)), one_hot, NN=True))\n",
    "        print('evaluation airlines')\n",
    "        airlines_rnn.append(model.evaluate(x_train_seq[test_index], y_train_oh[test_index])[1:])\n",
    "            \n",
    "amazon = pd.DataFrame({'NB':amazon_nb, 'SVM':amazon_svm, 'AdaBoost':amazon_adaboost, 'ANN':amazon_ann, 'RNN':amazon_rnn})\n",
    "amazon.to_csv('amazon_scores.csv')\n",
    "\n",
    "sentiment = pd.DataFrame({'NB':sentiment_nb, 'SVM':sentiment_svm, 'AdaBoost':sentiment_adaboost, 'ANN':sentiment_ann, 'RNN':sentiment_rnn})\n",
    "sentiment.to_csv('sentiment_scores.csv')         \n",
    "\n",
    "airlines = pd.DataFrame({'NB':airlines_nb, 'SVM':airlines_svm, 'AdaBoost':airlines_adaboost, 'ANN':airlines_ann, 'RNN':airlines_rnn})\n",
    "airlines.to_csv('airlines_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_bow\n",
    "\n",
    "amazon_nb = []\n",
    "amazon_svm = []\n",
    "amazon_adaboost = []\n",
    "amazon_ann = []\n",
    "amazon_rnn = []\n",
    "\n",
    "sentiment_nb = []\n",
    "sentiment_svm = []\n",
    "sentiment_adaboost = []\n",
    "sentiment_ann = []\n",
    "sentiment_rnn = []\n",
    "\n",
    "airlines_nb = []\n",
    "airlines_svm = []\n",
    "airlines_adaboost = []\n",
    "airlines_ann = []\n",
    "airlines_rnn = []\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "for k in range(5): #needs to average the results\n",
    "    for train_index, test_index in tqdm(kf.split(x_train), total=4):\n",
    "        indexes_amazon = np.random.choice(a=len(df_amazon), size=len(df_amazon), replace=False)\n",
    "        indexes_sentiment = np.random.choice(a=len(df_sentiment), size=len(df_sentiment), replace=False)\n",
    "        \n",
    "        # Naive Bayes\n",
    "        classifier = MultinomialNB()\n",
    "        classifier.fit(x_train[train_index], y_train[train_index])\n",
    "            \n",
    "        sentiment_nb.append(evaluate(df_sentiment[indexes_sentiment], classifier, vectorizer_bow.transform))\n",
    "        amazon_nb.append(evaluate(df_amazon[indexes_amazon], classifier, vectorizer_bow.transform))\n",
    "        airlines_nb.append(get_scores(y_train[test_index], classifier.predict(x_train[test_index])))\n",
    "        # SVM\n",
    "        classifier = SVC(kernel='rbf')\n",
    "        classifier.fit(x_train[train_index], y_train[train_index])\n",
    "            \n",
    "        sentiment_svm.append(evaluate(df_sentiment[indexes_sentiment], classifier, vectorizer_bow.transform))\n",
    "        amazon_svm.append(evaluate(df_amazon[indexes_amazon], classifier, vectorizer_bow.transform))\n",
    "        airlines_svm.append(get_scores(y_train[test_index], classifier.predict(x_train[test_index])))\n",
    "        # AdaBoost\n",
    "        classifier = AdaBoostClassifier(n_estimators=50, learning_rate=1.)\n",
    "        classifier.fit(x_train[train_index], y_train[train_index])\n",
    "            \n",
    "        sentiment_adaboost.append(evaluate(df_sentiment[indexes_sentiment], classifier, vectorizer_bow.transform))\n",
    "        amazon_adaboost.append(evaluate(df_amazon[indexes_amazon], classifier, vectorizer_bow.transform))\n",
    "        airlines_adaboost.append(get_scores(y_train[test_index], classifier.predict(x_train[test_index])))\n",
    "        \n",
    "        # ANN\n",
    "        model = FCN(x_train[train_index], y_train_oh[train_index])\n",
    "        sentiment_ann.append(evaluate(df_sentiment, model, vectorizer_bow.transform, one_hot))\n",
    "        amazon_ann.append(evaluate(df_amazon, model, vectorizer_bow.transform, one_hot))\n",
    "        airlines_ann.append(model.evaluate(x_train[test_index], y_train_oh[test_index])[1:])\n",
    "        \n",
    "amazon = pd.DataFrame({'NB':amazon_nb, 'SVM':amazon_svm, 'AdaBoost':amazon_adaboost, 'ANN':amazon_ann, 'RNN':amazon_rnn})\n",
    "amazon.to_csv('amazon_scores_bow.csv')\n",
    "\n",
    "sentiment = pd.DataFrame({'NB':sentiment_nb, 'SVM':sentiment_svm, 'AdaBoost':sentiment_adaboost, 'ANN':sentiment_ann, 'RNN':sentiment_rnn})\n",
    "sentiment.to_csv('sentiment_scores_bow.csv')         \n",
    "\n",
    "airlines = pd.DataFrame({'NB':airlines_nb, 'SVM':airlines_svm, 'AdaBoost':airlines_adaboost, 'ANN':airlines_ann, 'RNN':airlines_rnn})\n",
    "airlines.to_csv('airlines_scores_bow.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>NB</th>\n",
       "      <th>AdaBoost</th>\n",
       "      <th>ANN</th>\n",
       "      <th>RNN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.37908058 0.72377017 0.37908058]</td>\n",
       "      <td>[0.38163466 0.69889358 0.38163466]</td>\n",
       "      <td>[0.3414714  0.33778468 0.27975348 0.47843286]</td>\n",
       "      <td>[0.39540252 0.3956134  0.34360558 0.5118499 ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.39865833 0.71795989 0.39865833]</td>\n",
       "      <td>[0.32480262 0.71555708 0.32480262]</td>\n",
       "      <td>[0.3578629  0.36236638 0.27961123 0.48089302]</td>\n",
       "      <td>[0.42398864 0.4248059  0.3662462  0.5405918 ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[0.35288581 0.72538366 0.35288581]</td>\n",
       "      <td>[0.31620354 0.70879469 0.31620354]</td>\n",
       "      <td>[0.32674506 0.31738183 0.26180407 0.4595451 ]</td>\n",
       "      <td>[0.38192508 0.38320768 0.30238578 0.49419576]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[0.32262483 0.72892243 0.32262483]</td>\n",
       "      <td>[0.28804131 0.72233032 0.28804131]</td>\n",
       "      <td>[0.25516164 0.24041915 0.20474453 0.40125924]</td>\n",
       "      <td>[0.28835207 0.26595458 0.20429069 0.4157634 ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                  NB  \\\n",
       "0           0  [0.37908058 0.72377017 0.37908058]   \n",
       "1           1  [0.39865833 0.71795989 0.39865833]   \n",
       "2           2  [0.35288581 0.72538366 0.35288581]   \n",
       "3           3  [0.32262483 0.72892243 0.32262483]   \n",
       "\n",
       "                             AdaBoost  \\\n",
       "0  [0.38163466 0.69889358 0.38163466]   \n",
       "1  [0.32480262 0.71555708 0.32480262]   \n",
       "2  [0.31620354 0.70879469 0.31620354]   \n",
       "3  [0.28804131 0.72233032 0.28804131]   \n",
       "\n",
       "                                             ANN  \\\n",
       "0  [0.3414714  0.33778468 0.27975348 0.47843286]   \n",
       "1  [0.3578629  0.36236638 0.27961123 0.48089302]   \n",
       "2  [0.32674506 0.31738183 0.26180407 0.4595451 ]   \n",
       "3  [0.25516164 0.24041915 0.20474453 0.40125924]   \n",
       "\n",
       "                                             RNN  \n",
       "0  [0.39540252 0.3956134  0.34360558 0.5118499 ]  \n",
       "1  [0.42398864 0.4248059  0.3662462  0.5405918 ]  \n",
       "2  [0.38192508 0.38320768 0.30238578 0.49419576]  \n",
       "3  [0.28835207 0.26595458 0.20429069 0.4157634 ]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_amazon_scores=pd.read_csv('amazon_scores.csv')\n",
    "df_sentiment_scores=pd.read_csv('sentiment_scores.csv')\n",
    "df_airlines_scores=pd.read_csv('airlines_scores.csv')\n",
    "\n",
    "df_amazon_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a6eb94ab8ab9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mNB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_amazon_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mADA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_amazon_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdaBoost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mANN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_amazon_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mANN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mRNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_amazon_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-a6eb94ab8ab9>\u001b[0m in \u001b[0;36mclear\u001b[0;34m(list_of_strings)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplitted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0msplitted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplitted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtotal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplitted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: "
     ]
    }
   ],
   "source": [
    "def clear(list_of_strings):\n",
    "    total = []\n",
    "    for string in list_of_strings:\n",
    "        splitted = string.split(' ')\n",
    "        splitted[0] = splitted[0][1:]\n",
    "        splitted[-1] = splitted[-1][:-1]\n",
    "        \n",
    "        for k in range(len(splitted)):\n",
    "            splitted[k] = float(splitted[k])\n",
    "        \n",
    "        total.append(splitted)\n",
    "    return total\n",
    "\n",
    "classifiers_avg = np.zeros((4,4))\n",
    "np.asarray(df_amazon_scores.NB.values)\n",
    "\n",
    "NB = np.mean(clear(np.asarray(df_amazon_scores.NB.values)), axis=0)\n",
    "ADA = np.mean(clear(np.asarray(df_amazon_scores.AdaBoost.values)), axis=0)\n",
    "ANN = np.mean(clear(np.asarray(df_amazon_scores.ANN.values)), axis=0)\n",
    "RNN = np.mean(clear(np.asarray(df_amazon_scores.RNN.values)), axis=0)\n",
    "\n",
    "plt.figure()\n",
    "plt.bar([0,1,2,3], NB)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['[0.3414714  0.33778468 0.27975348 0.47843286]',\n",
       "       '[0.3578629  0.36236638 0.27961123 0.48089302]',\n",
       "       '[0.32674506 0.31738183 0.26180407 0.4595451 ]',\n",
       "       '[0.25516164 0.24041915 0.20474453 0.40125924]'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(df_amazon_scores.ANN.values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "107px",
    "width": "241px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
