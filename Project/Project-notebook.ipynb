{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening files and scouting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment140 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_raw_sentiment = pd.read_csv('data/sentiment140.csv', \n",
    "#                           engine='python', \n",
    "#                           header=None, \n",
    "#                           names=['classification', 'tweet_id', 'date', '?', 'account', 'text'])\n",
    "#df_raw_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sentiment = df_raw_sentiment[['classification', 'text']]\n",
    "#df_sentiment.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter Airline file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_raw_airlines = pd.read_csv('data/twitter-airline-sentiment.csv', engine='python')\n",
    "#df_raw_airlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_airlines = df_raw_airlines[['airline_sentiment', 'text']]\n",
    "#classification = np.zeros(len(df_airlines), dtype=np.int)\n",
    "#classification[df_airlines['airline_sentiment']=='negative'] = -1\n",
    "#classification[df_airlines['airline_sentiment']=='neutral'] = 0\n",
    "#classification[df_airlines['airline_sentiment']=='positive'] = 1\n",
    "#df_airlines.insert(loc=2, column='classification', value=classification)\n",
    "#df_airlines = df_airlines[['classification', 'text']]\n",
    "#df_airlines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re, string\n",
    "\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "def clean_single(tweet, stop_words = stopwords.words('english'), numbers=True):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "    # Removing links\n",
    "    tweet = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', tweet)\n",
    "    # Removing @\n",
    "    tweet = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", tweet)\n",
    "        \n",
    "    # Removing currencies\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    \n",
    "    tokens = word_tokenize(tweet)\n",
    "    \n",
    "    for token, tag in pos_tag(tokens):\n",
    "                \n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "            \n",
    "            # remove tokens containing numbers\n",
    "    if numbers:\n",
    "        numbers = [str(i) for i in range(10)]\n",
    "        kill_list = []\n",
    "        for number in numbers:\n",
    "            kill_list = kill_list + [w for w in cleaned_tokens if number in w]\n",
    "        # removing selected tokens\n",
    "        cleaned_tokens = [w for w in cleaned_tokens if not w in kill_list]\n",
    "        \n",
    "        # merge tokens\n",
    "        merged = ' '\n",
    "        merged = merged.join(cleaned_tokens)\n",
    "        \n",
    "    return merged\n",
    "\n",
    "def clean(array):\n",
    "    for i, phrase in enumerate(array):\n",
    "        array[i] = clean_single(phrase)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#airlines_classification = df_airlines.classification.values\n",
    "#airlines_text = clean(df_airlines.text.values)\n",
    "\n",
    "#sentiment_classification = df_sentiment.classification.values\n",
    "#sentiment_text = clean(df_sentiment.text.values)\n",
    "\n",
    "#print(airlines_classification.shape, airlines_text.shape)\n",
    "#print(sentiment_classification.shape, sentiment_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp = pd.DataFrame({'text':airlines_text,'classification':airlines_classification})\n",
    "#temp.to_csv('data/airlines_cleaned.csv', index=False)\n",
    "#temp = pd.DataFrame({'text':sentiment_text,'classification':sentiment_classification})\n",
    "#temp.to_csv('data/sentiment_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opening cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airlines = pd.read_csv('data/airlines_cleaned.csv')\n",
    "df_airlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment = pd.read_csv('data/sentiment_cleaned.csv')\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully connected network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "107px",
    "width": "241px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
