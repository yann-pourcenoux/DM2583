{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../data/lab_train.txt\", engine=\"python\")\n",
    "df_train.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "df_train.head() # 173 positive evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"../data/lab_test.txt\", engine=\"python\")\n",
    "df_test.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "df_test.head() # 88 positive evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.read_excel('../data/evaluation_dataset.xlsx', header=None, names=['review'])\n",
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews = df_train.review.values\n",
    "test_reviews = df_test.review.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "def clean_review(tokens, stop_words = stop_words, numbers=True):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "        \n",
    "    for token, tag in pos_tag(tokens):\n",
    "        # Removing links\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        # Removing @\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "            \n",
    "            # remove tokens containing numbers\n",
    "    if numbers:\n",
    "        numbers = [str(i) for i in range(10)]\n",
    "        kill_list = []\n",
    "        for number in numbers:\n",
    "            kill_list = kill_list + [w for w in cleaned_tokens if number in w]\n",
    "        # removing selected tokens\n",
    "        cleaned_tokens = [w for w in cleaned_tokens if not w in kill_list]\n",
    "        \n",
    "        # merge tokens\n",
    "        merged = ' '\n",
    "        merged = merged.join(cleaned_tokens)\n",
    "        \n",
    "    return merged\n",
    "\n",
    "def clean(array):\n",
    "    for i, phrase in enumerate(array):\n",
    "        array[i] = clean_review(word_tokenize(phrase))\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews = clean(train_reviews)\n",
    "test_reviews = clean(test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=5,\n",
    "                             stop_words=stopwords.words('english'),\n",
    "                             strip_accents='ascii')\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_reviews).toarray()\n",
    "X_test = vectorizer.transform(test_reviews).toarray()\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def targets(score, thresh=3):\n",
    "    targets = np.ones(score.shape, dtype=np.int)\n",
    "    targets = targets - 2*((score<thresh).astype(dtype=np.int))\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = targets(df_train.score.values)\n",
    "Y_test = targets(df_test.score.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to reduce influence of positive influence\n",
    "neg = Y_train == -1\n",
    "pos = Y_train == 1\n",
    "\n",
    "# new_x_train = np.concatenate((X_train[neg], X_train[pos][:90]))\n",
    "# new_y_train = np.concatenate((Y_train[neg], Y_train[pos][:90]))\n",
    "\n",
    "classifiers=[]\n",
    "scores = []\n",
    "for k in range(2000):\n",
    "    idxs = choice(173, 27, replace=False)\n",
    "    \n",
    "    x_array = [X_train[neg]]+[X_train[idxs]]\n",
    "    y_array = [Y_train[neg]]+[Y_train[idxs]]\n",
    "    \n",
    "    new_x_train = np.concatenate(x_array)\n",
    "    new_y_train = np.concatenate(y_array)\n",
    "    \n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(new_x_train, new_y_train)\n",
    "    Y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    X_eval = vectorizer.transform(df_eval.review.values)\n",
    "    score = classifier.predict(X_eval)\n",
    "    \n",
    "    classifiers.append(classifier)\n",
    "    scores.append(f1_score(Y_test,Y_pred))\n",
    "    \n",
    "#     print(k)\n",
    "#     print(classification_report(Y_test, Y_pred))\n",
    "    \n",
    "print(scores[scores.index(max(scores))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifiers[scores.index(max(scores))]\n",
    "\n",
    "# Compute the predictions\n",
    "Y_pred = classifier.predict(X_test)\n",
    "print('Accuracy on test data is:', np.mean(Y_pred==Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eval = vectorizer.transform(df_eval.review.values)\n",
    "pred_eval = classifier.predict(X_eval)\n",
    "np.sum(pred_eval==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.zeros(pred_eval.shape, dtype='O')\n",
    "predictions[pred_eval==1] = 'positive'\n",
    "predictions[pred_eval==-1] = 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.read_excel('../data/evaluation_dataset.xlsx', header=None, names=['review'])\n",
    "df_eval.insert(1, \"prediction\", predictions, True)\n",
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.to_csv('../data/evaluation_cleaning.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
